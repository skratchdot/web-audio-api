<dl title="interface BaseAudioContext : EventTarget" class="idl" data-merge="DecodeSuccessCallback DecodeErrorCallback">
          <dt>
            readonly attribute AudioDestinationNode destination
          </dt>
          <dd>
            <p>
              An <a href="#AudioDestinationNode"><code>AudioDestinationNode</code></a>
              with a single input representing the final destination for all
              audio. Usually this will represent the actual audio hardware. All
              <a><code>AudioNode</code></a>s actively rendering audio will
              directly or indirectly connect to <a href="#widl-BaseAudioContext-destination"><code>destination</code></a>.
            </p>
          </dd>
          <dt>
            readonly attribute float sampleRate
          </dt>
          <dd>
            <p>
              The sample rate (in sample-frames per second) at which the
              <a><code>BaseAudioContext</code></a> handles audio. It is assumed
              that all <a><code>AudioNode</code></a>s in the context run at
              this rate. In making this assumption, sample-rate converters or
              &quot;varispeed&quot; processors are not supported in real-time processing.
              The <dfn>Nyquist frequency</dfn> is half this sample-rate value.
            </p>
          </dd>
          <dt>
            readonly attribute double currentTime
          </dt>
          <dd>
            <p>
              This is the time in seconds of the sample frame immediately
              following the last sample-frame in the block of audio most
              recently processed by the context&apos;s rendering graph. If the
              context&apos;s rendering graph has not yet processed a block of audio,
              then <a href="#widl-BaseAudioContext-currentTime"><code>currentTime</code></a>
              has a value of zero.
            </p>
            <p>
              In the time coordinate system of <a href="#widl-BaseAudioContext-currentTime"><code>currentTime</code></a>,
              the value of zero corresponds to the first sample-frame in the
              first block processed by the graph. Elapsed time in this system
              corresponds to elapsed time in the audio stream generated by the
              <a><code>BaseAudioContext</code></a>, which may not be
              synchronized with other clocks in the system. (For an
              <a><code>OfflineAudioContext</code></a>, since the stream is not
              being actively played by any device, there is not even an
              approximation to real time.)
            </p>
            <p>
              All scheduled times in the Web Audio API are relative to the
              value of <a><code>currentTime</code></a>.
            </p>
            <p>
              When the <a><code>BaseAudioContext</code></a> is in the <a href="#idl-def-AudioContextState.running"><code>running</code></a>
              state, the value of this attribute is monotonically increasing
              and is updated by the rendering thread in uniform increments,
              corresponding to the audio block size of 128 samples. Thus, for a
              running context, <code>currentTime</code> increases steadily as
              the system processes audio blocks, and always represents the time
              of the start of the next audio block to be processed. It is also
              the earliest possible time when any change scheduled in the
              current state might take effect.
            </p>
          </dd>
          <dt>
            readonly attribute AudioListener listener
          </dt>
          <dd>
            <p>
              An <a href="#AudioListener"><code>AudioListener</code></a> which
              is used for 3D <a href="#Spatialization">spatialization</a>.
            </p>
          </dd>
          <dt>
            readonly attribute AudioContextState state
          </dt>
          <dd>
            Describes the current state of the <a>AudioContext</a>, on the
            <a>control thread</a>.
          </dd>
          <dt>
            readonly attribute double baseLatency
          </dt>
          <dd>
            <p>
              The actual processing latency of the AudioContext. This
              represents the number of seconds of processing latency incurred
              by the AudioContext in handling audio through the graph. It does
              not include any additional latency that might be caused by any
              other processing between the output of the
              <a>AudioDestinationNode</a> and the audio hardware.
            </p>
            <p>
              For example, if the audio context is running at 44.1 kHz and the
              <a>AudioDestinationNode</a> implements double buffering
              internally and can process and output audio every 128 sample
              frames, then the processing latency is \((2\cdot128)/44100 =
              5.805 \mathrm{ ms}\), approximately.
            </p>
          </dd>
          <dt>
            Promise&lt;void&gt; suspend()
          </dt>
          <dd>
            <p>
              Suspends the progression of
              <a><code>BaseAudioContext</code></a>&apos;s <a href="#widl-BaseAudioContext-currentTime">currentTime</a>, allows any
              current context processing blocks that are already processed to
              be played to the destination, and then allows the system to
              release its claim on audio hardware. This is generally useful
              when the application knows it will not need the
              <a>BaseAudioContext</a> for some time, and wishes to temporarily
              <a>release system resource</a> associated with the
              <a>BaseAudioContext</a>. The promise resolves when the frame
              buffer is empty (has been handed off to the hardware), or
              immediately (with no other effect) if the context is already
              <code>suspended</code>. The promise is rejected if the context
              has been closed.
            </p>
            <p>
              <span class="synchronous">When suspend is called, execute these
              steps:</span>
            </p>
            <ol>
              <li>Let <em>promise</em> be a new Promise.
              </li>
              <li>If the <em>control thread state</em> flag on the
              <a>AudioContext</a> is <code>closed</code> reject the promise
              with <code>InvalidStateError</code>, abort these steps, returning
              <em>promise</em>.
              </li>
              <li>If the <a href="#widl-AudioContext-state">state</a> attribute
              of the <a>AudioContext</a> is already <code>suspended</code>,
              resolve <em>promise</em>, return it, and abort these steps.
              </li>
              <li>Set the <em>control thread state</em> flag on the
              <a>AudioContext</a> to <code>suspended</code>.
              </li>
              <li>
                <a href="#queue">Queue a control message</a> to suspend the
                <a>AudioContext</a>.
              </li>
              <li>Return <em>promise</em>.
              </li>
            </ol>
            <p>
              Running a <a>control message</a> to suspend an
              <a>AudioContext</a> means running these steps on the <a>rendering
              thread</a>:
            </p>
            <ol>
              <li>Attempt to <a>release system resources</a>.
              </li>
              <li>Queue a task on the <a>control thread</a>&apos;s event loop, to
              execute these steps:
                <ol>
                  <li>Resolve <em>promise</em>.
                  </li>
                  <li>If the <a href="#widl-audiocontext-state">state</a>
                  attribute of the <a>AudioContext</a> is not already
                  <code>suspended</code>:
                    <ol>
                      <li>Set the <a href="#widl-audiocontext-state">state</a>
                      attribute of the <a>AudioContext</a> to
                      <code>suspended</code>.
                      </li>
                      <li>Queue a task to fire a simple event named
                      <code>statechanged</code> at the <a>AudioContext</a>
                      </li>
                    </ol>
                  </li>
                </ol>
              </li>
            </ol>
            <p>
              While a <a>BaseAudioContext</a> is suspended,
              <code>MediaStream</code>s will have their output ignored; that
              is, data will be lost by the real time nature of media streams.
              <code>HTMLMediaElement</code>s will similarly have their output
              ignored until the system is resumed. <a>AudioWorker</a>s and
              <a>ScriptProcessorNode</a>s will simply not fire their
              <code>onaudioprocess</code> events while suspended, but will
              resume when resumed. For the purpose of <a>AnalyserNode</a>
              window functions, the data is considered as a continuous stream -
              i.e. the <code>resume()</code>/<code>suspend()</code> does not
              cause silence to appear in the <a>AnalyserNode</a>&apos;s stream of
              data. In particular, calling <a>AnalyserNode</a> functions
              repeatedly when a <a>BaseAudioContext</a> is suspended MUST
              return the same data.
            </p>
          </dd>
          <dt>
            Promise&lt;void&gt; resume()
          </dt>
          <dd>
            Resumes the progression of the <a><code>AudioContext</code></a>&apos;s&apos;
            <a href="#widl-AudioContext-currentTime">currentTime</a> when it
            has been suspended.
            <p>
              <span class="synchronous">When resume is called, execute these
              steps:</span>
            </p>
            <ol>
              <li>Let <em>promise</em> be a new Promise.
              </li>
              <li>If the <em>control thread state</em> flag on the
              <a>AudioContext</a> is <code>closed</code> reject the promise
              with <code>InvalidStateError</code>, abort these steps, returning
              <em>promise</em>.
              </li>
              <li>If the <a href="#widl-AudioContext-state">state</a> attribute
              of the <a>AudioContext</a> is already <code>running</code>,
              resolve <em>promise</em>, return it, and abort these steps.
              </li>
              <li>Set the <em>control thread state</em> flag on the
              <a>AudioContext</a> to <code>running</code>.
              </li>
              <li>
                <a href="#queue">Queue a control message</a> to resume the
                <a>AudioContext</a>.
              </li>
              <li>Return <em>promise</em>.
              </li>
            </ol>
            <p>
              Running a <a>control message</a> to resume an <a>AudioContext</a>
              means running these steps on the <a>rendering thread</a>:
            </p>
            <ol>
              <li>Attempt to <a href="#acquiring">acquire system resources</a>.
              </li>
              <li>In case of failure, queue a task on the <a>control thread</a>
              to execute the following, and abort these steps
                <ol>
                  <li>Reject <em>promise</em>
                  </li>
                </ol>
              </li>
              <li>Queue a task on the <a>control thread</a>&apos;s event loop, to
              execute these steps:
                <ol>
                  <li>Resolve <em>promise</em>.
                  </li>
                  <li>If the <a href="#widl-audiocontext-state">state</a>
                  attribute of the <a>AudioContext</a> is not already
                  <code>running</code>:
                    <ol>
                      <li>Set the <a href="#widl-audiocontext-state">state</a>
                      attribute of the <a>AudioContext</a> to
                      <code>running</code>.
                      </li>
                      <li>Queue a task to fire a simple event named
                      <code>statechange</code> at the <a>AudioContext</a>
                      </li>
                    </ol>
                  </li>
                </ol>
              </li>
            </ol>
          </dd>
          <dt>
            Promise&lt;void&gt; close()
          </dt>
          <dd>
            Closes the <a>BaseAudioContext</a>, <a>releasing the system
            resources</a> it&apos;s using. This will not automatically release all
            <a>AudioContext</a>-created objects, but will suspend the
            progression of the <a><code>AudioContext</code></a>&apos;s <a href="#widl-AudioContext-currentTime">currentTime</a>, and stop
            processing audio data.
            <p>
              <span class="synchronous">When close is called, execute these
              steps:</span>
            </p>
            <ol>
              <li>Let <em>promise</em> be a new Promise.
              </li>
              <li>If this <a>BaseAudioContext</a> is an
              <a>OfflineAudioContext</a>, reject <em>promise</em> with
              <code>NotSupportedError</code>, return it, and abort these steps.
              </li>
              <li>If the <em>control thread state</em> flag on the
              <a>AudioContext</a> is <code>closed</code> reject the promise
              with <code>InvalidStateError</code>, abort these steps, returning
              <em>promise</em>.
              </li>
              <li>If the <a href="#widl-AudioContext-state">state</a> attribute
              of the <a>AudioContext</a> is already <code>closed</code>,
              resolve <em>promise</em>, return it, and abort these steps.
              </li>
              <li>Set the <em>control thread state</em> flag on the
              <a>AudioContext</a> to <code>closed</code>.
              </li>
              <li>
                <a href="#queue">Queue a control message</a> to the
                <a>AudioContext</a>.
              </li>
              <li>Return <em>promise</em>.
              </li>
            </ol>
            <p>
              Running a <a>control message</a> to close an
              <a>BaseAudioContext</a> means running these steps on the
              <a>rendering thread</a>:
            </p>
            <ol>
              <li>Attempt to <a>release system resources</a>.
              </li>
              <li>Queue a task on the <a>control thread</a>&apos;s event loop, to
              execute these steps:
                <ol>
                  <li>Resolve <em>promise</em>.
                  </li>
                  <li>If the <a href="#widl-audiocontext-state">state</a>
                  attribute of the <a>AudioContext</a> is not already
                  <code>closed</code>:
                    <ol>
                      <li>Set the <a href="#widl-audiocontext-state">state</a>
                      attribute of the <a>AudioContext</a> to
                      <code>closed</code>.
                      </li>
                      <li>Queue a task to fire a simple event named
                      <code>statechange</code> at the <a>AudioContext</a>
                      </li>
                    </ol>
                  </li>
                </ol>
              </li>
            </ol>
            <p class="note">
              When an <a>BaseAudioContext</a> has been closed, implementation
              can choose to aggressively release more resources than when
              suspending.
            </p>
          </dd>
          <dt>
            attribute EventHandler onstatechange
          </dt>
          <dd>
            A property used to set the <code>EventHandler</code> for an event
            that is dispatched to <a><code>BaseAudioContext</code></a> when the
            state of the AudioContext has changed (i.e. when the corresponding
            promise would have resolved). An event of type
            <a><code>Event</code></a> will be dispatched to the event handler,
            which can query the AudioContext&apos;s state directly. A newly-created
            AudioContext will always begin in the <code>suspended</code> state,
            and a state change event will be fired whenever the state changes
            to a different state. This event is fired before the
            <a><code>oncomplete</code></a> event is fired.
          </dd>
          <dt>
            AudioBuffer createBuffer()
          </dt>
          <dd>
            Creates an AudioBuffer of the given size. The audio data in the
            buffer will be zero-initialized (silent). <span class="synchronous">A NotSupportedError exception MUST be thrown if any
            of the arguments is negative, zero, or outside its nominal
            range.</span>
            <dl class="parameters">
              <dt>
                unsigned long numberOfChannels
              </dt>
              <dd>
                Determines how many channels the buffer will have. An
                implementation must support at least 32 channels.
              </dd>
              <dt>
                unsigned long length
              </dt>
              <dd>
                Determines the size of the buffer in sample-frames.
              </dd>
              <dt>
                float sampleRate
              </dt>
              <dd>
                Describes the sample-rate of the linear PCM audio data in the
                buffer in sample-frames per second. An implementation must
                support sample rates in at least the range 8000 to 96000.
              </dd>
            </dl>
          </dd>
          <dt>
            Promise&lt;AudioBuffer&gt; decodeAudioData()
          </dt>
          <dd>
            Asynchronously decodes the audio file data contained in the
            <code>ArrayBuffer</code>. The <code>ArrayBuffer</code> can, for
            example, be loaded from an <code>XMLHttpRequest</code>&apos;s
            <code>response</code> attribute after setting the
            <code>responseType</code> to <code>&quot;arraybuffer&quot;</code>. Audio file
            data can be in any of the formats supported by the
            <code>audio</code> element. The buffer passed to <a href="#widl-AudioContext-decodeAudioData-Promise-AudioBuffer--ArrayBuffer-audioData-DecodeSuccessCallback-successCallback-DecodeErrorCallback-errorCallback">
            decodeAudioData</a> has its content-type determined by sniffing, as
            described in [[mimesniff]].
            <dl class="parameters">
              <dt>
                ArrayBuffer audioData
              </dt>
              <dd>
                An ArrayBuffer containing compressed audio data
              </dd>
              <dt>
                optional DecodeSuccessCallback successCallback
              </dt>
              <dd>
                A callback function which will be invoked when the decoding is
                finished. The single argument to this callback is an
                AudioBuffer representing the decoded PCM audio data.
              </dd>
              <dt>
                optional DecodeErrorCallback errorCallback
              </dt>
              <dd>
                A callback function which will be invoked if there is an error
                decoding the audio file.
              </dd>
            </dl>
            <p>
              Although the primary method of interfacing with this function is
              via its promise return value, the callback parameters are
              provided for legacy reasons. The system shall ensure that the
              <a>AudioContext</a> is not garbage collected before the promise
              is resolved or rejected and any callback function is called and
              completes.
            </p>
            <p>
              <span class="synchronous">When <code>decodeAudioData</code> is
              called, the following steps must be performed on the control
              thread:</span>
            </p>
            <ol>
              <li>Let <var>promise</var> be a new promise.
              </li>
              <li>
                <a href="https://tc39.github.io/ecma262/#sec-detacharraybuffer">Detach</a>
                the <a>audioData</a> <code>ArrayBuffer</code>. This operation
                is described in [[!ECMASCRIPT]].
              </li>
              <li>Queue a decoding operation to be performed on another thread.
              </li>
              <li>Return <var>promise</var>.
              </li>
            </ol>
            <p>
              When queuing a decoding operation to be performed on another
              thread, the following steps MUST happen on a thread that is not
              the <a>control thread</a> nor the <a>rendering thread</a>, called
              the <em>decoding thread</em>.
            </p>
            <div class="note">
              Multiple <em>decoding threads</em> can run in parallel to service
              multiple calls to <code>decodeAudioData</code>.
            </div>
            <ol>
              <li>Attempt to decode the encoded <a>audioData</a> into linear
              PCM.
              </li>
              <li>If a decoding error is encountered due to the audio format
              not being recognized or supported, or because of
              corrupted/unexpected/inconsistent data, then queue a task to
              execute the following step, on the <a>control thread</a>&apos;s event
              loop:
                <ol>
                  <li>Let <var>error</var> be a <code>DOMException</code> whose
                  name is <code>&quot;EncodingError&quot;</code>.
                  </li>
                  <li>Reject <var>promise</var> with <var>error</var>.
                  </li>
                  <li>If <dfn>errorCallback</dfn> is not missing, invoke
                  <a>errorCallback</a> with <var>error</var>.
                  </li>
                </ol>
              </li>
              <li>Otherwise:
                <ol>
                  <li>Take the result, representing the decoded linear PCM
                  audio data, and resample it to the sample-rate of the
                  <a><code>AudioContext</code></a> if it is different from the
                  sample-rate of <a>audioData</a>.
                  </li>
                  <li>Queue a task on the <a>control thread</a>&apos;s event loop to
                  execute the following steps:
                    <ol>
                      <li>Let <var>buffer</var> be an <code>AudioBuffer</code>
                      containing the final result (after possibly sample-rate
                      conversion).
                      </li>
                      <li>Resolve <var>promise</var> with <var>buffer</var>.
                      </li>
                      <li>If <dfn>successCallback</dfn> is not missing, invoke
                      <a>successCallback</a> with <var>buffer</var>.
                      </li>
                    </ol>
                  </li>
                </ol>
              </li>
            </ol>
          </dd>
          <dt>
            AudioBufferSourceNode createBufferSource()
          </dt>
          <dd>
            Creates an <a><code>AudioBufferSourceNode</code></a>.
            <a>AudioBufferSourceNode</a> are created with an internal flag
            <code>buffer set</code>, initially set to false.
          </dd>
          <dt>
            Promise&lt;AudioWorker&gt; createAudioWorker()
          </dt>
          <dd>
            Creates an <a><code>AudioWorker</code></a> object and loads the
            associated script into an
            <a><code>AudioWorkerGlobalScope</code></a>, then resolves the
            returned Promise.
            <dl class="parameters">
              <dt>
                DOMString scriptURL
              </dt>
              <dd>
                This parameter represents the URL of the script to be loaded as
                an <a>AudioWorker</a> node factory. See <a>AudioWorker</a>
                section for more detail.
              </dd>
            </dl>
          </dd>
          <dt>
            ScriptProcessorNode createScriptProcessor()
          </dt>
          <dd>
            This method is DEPRECATED, as it is intended to be replaced by
            createAudioWorker. Creates a
            <a><code>ScriptProcessorNode</code></a> for direct audio processing
            using JavaScript. <span class="synchronous">An IndexSizeError
            exception MUST be thrown if <a><code>bufferSize</code></a> or
            <a><code>numberOfInputChannels</code></a> or
            <a><code>numberOfOutputChannels</code></a> are outside the valid
            range.</span>
            <dl class="parameters">
              <dt>
                optional unsigned long bufferSize = 0
              </dt>
              <dd>
                The <a><code>bufferSize</code></a> parameter determines the
                buffer size in units of sample-frames. If it&apos;s not passed in,
                or if the value is 0, then the implementation will choose the
                best buffer size for the given environment, which will be
                constant power of 2 throughout the lifetime of the node.
                Otherwise if the author explicitly specifies the bufferSize, it
                must be one of the following values: 256, 512, 1024, 2048,
                4096, 8192, 16384. This value controls how frequently the
                <code>audioprocess</code> event is dispatched and how many
                sample-frames need to be processed each call. Lower values for
                <a><code>bufferSize</code></a> will result in a lower (better)
                <a href="#latency">latency</a>. Higher values will be necessary
                to avoid audio breakup and <a href="#audio-glitching">glitches</a>. It is recommended for authors
                to not specify this buffer size and allow the implementation to
                pick a good buffer size to balance between <a href="#latency">latency</a> and audio quality. If the value of this
                parameter is not one of the allowed power-of-2 values listed
                above, an IndexSizeError MUST be thrown.
              </dd>
              <dt>
                optional unsigned long numberOfInputChannels = 2
              </dt>
              <dd>
                This parameter determines the number of channels for this
                node&apos;s input. Values of up to 32 must be supported.
              </dd>
              <dt>
                optional unsigned long numberOfOutputChannels = 2
              </dt>
              <dd>
                This parameter determines the number of channels for this
                node&apos;s output. Values of up to 32 must be supported.
              </dd>
            </dl>It is invalid for both
            <a><code>numberOfInputChannels</code></a> and
            <a><code>numberOfOutputChannels</code></a> to be zero. In this case
            an IndexSizeError MUST be thrown.
          </dd>
          <dt>
            AnalyserNode createAnalyser()
          </dt>
          <dd>
            Create an <a><code>AnalyserNode</code></a>.
          </dd>
          <dt>
            GainNode createGain()
          </dt>
          <dd>
            Create an <a><code>GainNode</code></a>.
          </dd>
          <dt>
            DelayNode createDelay()
          </dt>
          <dd>
            Creates a <a><code>DelayNode</code></a> representing a variable
            delay line. The initial default delay time will be 0 seconds.
            <dl class="parameters">
              <dt>
                optional double maxDelayTime = 1.0
              </dt>
              <dd>
                The <dfn>maxDelayTime</dfn> parameter is optional and specifies
                the maximum delay time in seconds allowed for the delay line.
                <span class="synchronous">If specified, this value MUST be
                greater than zero and less than three minutes or a
                NotSupportedError exception MUST be thrown.</span>
              </dd>
            </dl>
          </dd>
          <dt>
            BiquadFilterNode createBiquadFilter()
          </dt>
          <dd>
            Creates a <a><code>BiquadFilterNode</code></a> representing a
            second order filter which can be configured as one of several
            common filter types.
          </dd>
          <dt>
            IIRFilterNode createIIRFilter(sequence&lt;double&gt; b,
            sequence&lt;double&gt; a)
          </dt>
          <dd>
            Creates an <a><code>IIRFilterNode</code></a> representing a general
            IIR Filter.
            <dl class="parameters">
              <dt>
                sequence&lt;double&gt; feedforward
              </dt>
              <dd>
                An array of the feedforward (numerator) coefficients for the
                transfer function of the IIR filter. The maximum length of this
                array is 20. If all of the values are zero, an
                InvalidStateError MUST be thrown. A NotSupportedError MUST be
                thrown if the array length is 0 or greater than 20.
              </dd>
              <dt>
                sequence&lt;double&gt; feedback
              </dt>
              <dd>
                An array of the feedback (denominator) coefficients for the
                tranfer function of the IIR filter. The maximum length of this
                array is 20. If the first element of the array is 0, an
                InvalidStateError MUST be thrown. A NotSupportedError MUST be
                thrown if the array length is 0 or greater than 20.
              </dd>
            </dl>
          </dd>
          <dt>
            WaveShaperNode createWaveShaper()
          </dt>
          <dd>
            Creates a <a><code>WaveShaperNode</code></a> representing a
            non-linear distortion.
          </dd>
          <dt>
            PannerNode createPanner()
          </dt>
          <dd>
            Creates a <a><code>PannerNode</code></a>.
          </dd>
          <dt>
            StereoPannerNode createStereoPanner()
          </dt>
          <dd>
            Creates a <a><code>StereoPannerNode</code></a>.
          </dd>
          <dt>
            ConvolverNode createConvolver()
          </dt>
          <dd>
            Creates a <a><code>ConvolverNode</code></a>.
          </dd>
          <dt>
            ChannelSplitterNode createChannelSplitter()
          </dt>
          <dd>
            Creates an <a><code>ChannelSplitterNode</code></a> representing a
            channel splitter. <span class="synchronous">An IndexSizeError
            exception MUST be thrown for invalid parameter values.</span>
            <dl class="parameters">
              <dt>
                optional unsigned long numberOfOutputs = 6
              </dt>
              <dd>
                The number of outputs. Values of up to 32 must be supported. If
                not specified, then 6 will be used.
              </dd>
            </dl>
          </dd>
          <dt>
            ChannelMergerNode createChannelMerger()
          </dt>
          <dd>
            Creates a <a><code>ChannelMergerNode</code></a> representing a
            channel merger. <span class="synchronous">An IndexSizeError
            exception MUST be thrown for invalid parameter values.</span>
            <dl class="parameters">
              <dt>
                optional unsigned long numberOfInputs = 6
              </dt>
              <dd>
                The <dfn>numberOfInputs</dfn> parameter determines the number
                of inputs. Values of up to 32 must be supported. If not
                specified, then 6 will be used.
              </dd>
            </dl>
          </dd>
          <dt>
            DynamicsCompressorNode createDynamicsCompressor()
          </dt>
          <dd>
            Creates a <a><code>DynamicsCompressorNode</code></a>
          </dd>
          <dt>
            OscillatorNode createOscillator()
          </dt>
          <dd>
            Creates an <a><code>OscillatorNode</code></a>
          </dd>
          <dt>
            PeriodicWave createPeriodicWave()
          </dt>
          <dd>
            Creates a <a><code>PeriodicWave</code></a> representing a waveform
            containing arbitrary harmonic content. <span class="synchronous">The <code>real</code> and <code>imag</code>
            parameters must be of type <code>Float32Array</code> (described in
            [[!TYPED-ARRAYS]]) of equal lengths greater than zero or an
            IndexSizeError exception MUST be thrown.</span> All implementations
            must support arrays up to at least 8192. These parameters specify
            the Fourier coefficients of a <a href="https://en.wikipedia.org/wiki/Fourier_series">Fourier series</a>
            representing the partials of a periodic waveform. The created
            <a><code>PeriodicWave</code></a> will be used with an
            <a><code>OscillatorNode</code></a> and, by default, will represent
            a <em>normalized</em> time-domain waveform having maximum absolute
            peak value of 1. Another way of saying this is that the generated
            waveform of an <a><code>OscillatorNode</code></a> will have maximum
            peak value at 0dBFS. Conveniently, this corresponds to the
            full-range of the signal values used by the Web Audio API. Because
            the PeriodicWave is normalized by default on creation, the
            <code>real</code> and <code>imag</code> parameters represent
            <em>relative</em> values. If normalization is disabled via the
            <code>disableNormalization</code> parameter, this normalization is
            disabled, and the time-domain waveform has the amplitudes as given
            by the Fourier coefficients.
            <p>
              As <a>PeriodicWave</a> objects maintain their own copies of these
              arrays, any modification of the arrays uses as the
              <code>real</code> and <code>imag</code> parameters after the call
              to <a href="#widl-BaseAudioContext-createPeriodicWave-PeriodicWave-Float32Array-real-Float32Array-imag-PeriodicWaveConstraints-constraints">
              createPeriodicWave()</a> will have no effect on the
              <a>PeriodicWave</a> object.
            </p>
            <dl class="parameters">
              <dt>
                Float32Array real
              </dt>
              <dd>
                The <dfn id="dfn-real">real</dfn> parameter represents an array
                of <code>cosine</code> terms (traditionally the A terms). In
                audio terminology, the first element (index 0) is the DC-offset
                of the periodic waveform. The second element (index 1)
                represents the fundamental frequency. The third element
                represents the first overtone, and so on. The first element is
                ignored and implementations must set it to zero internally.
              </dd>
              <dt>
                Float32Array imag
              </dt>
              <dd>
                The <dfn id="dfn-imag">imag</dfn> parameter represents an array
                of <code>sine</code> terms (traditionally the B terms). The
                first element (index 0) should be set to zero (and will be
                ignored) since this term does not exist in the Fourier series.
                The second element (index 1) represents the fundamental
                frequency. The third element represents the first overtone, and
                so on.
              </dd>
              <dt>
                optional PeriodicWaveConstraints constraints
              </dt>
              <dd>
                If not given, the waveform is normalized. Otherwise, the
                waveform is normalized according the value given by
                <code>constraints</code>.
              </dd>
            </dl>
          </dd>
        </dl>